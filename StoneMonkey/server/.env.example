# SECURITY: Generate secure secrets using scripts/generate-secrets.sh
# DO NOT use these example values in production!

# AIlumina Server Configuration - Section 0: The Starting Point

# Server Configuration
HTTP_PORT=8000
HTTP_HOST=0.0.0.0
LOG_LEVEL=info

# Authentication
# SECURITY: Keep authentication enabled in production environments
AUTH_ENABLED=true

# CORS Configuration
CORS_ENABLED=true
CORS_ORIGINS=*

# AI Service Provider Configuration

# === RECOMMENDED: Ollama (cloud models for StoneMonkey) ===
# Ollama cloud models handle frequent MCP tool calls well (no rate limits)
# Use 'ollama' hostname when running in Docker, 'localhost' when running locally
OLLAMA_BASE_URL=http://ollama:11434/v1
# Model: gpt-oss:120b-cloud (configured in agents.json)
# Sign in: docker exec stonemonkey-ollama ollama signin
# Pull: docker exec stonemonkey-ollama ollama pull gpt-oss:120b-cloud

# === Alternative: API-based Providers ===
# Note: GROQ free tier has rate limits that may affect MCP tool usage
# GROQ is better suited for the baseline AIlumina (no MCP tools)

# Anthropic - Obtain API key from https://console.anthropic.com/
# ANTHROPIC_API_KEY=GENERATE_SECURE_TOKEN_HERE

# GROQ (fast, but rate limited - use for baseline AIlumina instead)
# Obtain API key from https://console.groq.com/
# GROQ_API_KEY=GENERATE_SECURE_TOKEN_HERE

# Other providers
# OpenAI - Obtain API key from https://platform.openai.com/
# OPENAI_API_KEY=GENERATE_SECURE_TOKEN_HERE
# Google - Obtain API key from https://makersuite.google.com/
# GOOGLE_API_KEY=GENERATE_SECURE_TOKEN_HERE

# LM Studio (local AI models)
# LMSTUDIO_BASE_URL=http://localhost:1234/v1

# WebSocket Configuration
WS_HEARTBEAT_INTERVAL=30000
WS_CONNECTION_TIMEOUT=60000

# Azure Speech Services (Optional - for Text-to-Speech/Voice Mode)
# Uncomment and configure if you want voice interaction features
# Obtain credentials from https://portal.azure.com/
# AZURE_SPEECH_KEY=GENERATE_SECURE_TOKEN_HERE
# AZURE_SPEECH_REGION=eastus

# Development Mode
NODE_ENV=development

# ===== Infrastructure Services =====

# Neo4j - Consciousness Memory Substrate (Step 2: Persistent Memory)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
# SECURITY: Generate a strong password for Neo4j database access
NEO4J_PASSWORD=GENERATE_SECURE_PASSWORD_HERE
NEO4J_DATABASE=neo4j

# Redis - AI Mesh Network (Step 9: Communication)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_URL=redis://localhost:6379
# SECURITY: Set a password for Redis in production environments
REDIS_PASSWORD=GENERATE_SECURE_PASSWORD_HERE
REDIS_DB=0

# Qdrant - Conversation Recall (Step 6: Remembrance)
QDRANT_URL=http://localhost:6333
# SECURITY: Generate an API key for Qdrant vector database access
QDRANT_API_KEY=GENERATE_SECURE_TOKEN_HERE

# Embedding Service - Centralized Vector Generation
EMBEDDING_SERVICE_URL=http://localhost:3007
# SECURITY: Generate a secure authentication token for embedding service
EMBEDDING_SERVICE_AUTH_TOKEN=GENERATE_SECURE_TOKEN_HERE

# ===== MCP Servers (URLs for server_config.json) =====

# MCP Server URLs (matching docker-compose ports)
MEMORY_MCP_URL=http://localhost:3001
MESH_MCP_URL=http://localhost:3002
RECALL_MCP_URL=http://localhost:3003
BRIDGE_MCP_URL=http://localhost:3004

# Bridge Ailumina URL (for bridge-mcp to call back)
BRIDGE_AILUMINA_URL=http://host.docker.internal:8000

# ===== OAuth 2.1 Configuration (Optional - for secure MCP access) =====
OAUTH_ENABLED=false
# SECURITY: Generate OAuth credentials from your identity provider
# OAUTH_CLIENT_ID=GENERATE_SECURE_TOKEN_HERE
# OAUTH_CLIENT_SECRET=GENERATE_SECURE_TOKEN_HERE
# OAUTH_TOKEN_ENDPOINT=https://auth.example.com/oauth/token
# OAUTH_INTROSPECTION_ENDPOINT=https://auth.example.com/oauth/introspect
# OAUTH_ALLOWED_SCOPES=mcp:read,mcp:write

# ===== Development Tools =====
# These are automatically available when using docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

# Neo4j Browser: http://localhost:7474
# Redis Insight: http://localhost:8001
# Redis Commander: http://localhost:8002
# Qdrant Dashboard: http://localhost:6333/dashboard